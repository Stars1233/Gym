(env-user-modeling)=
# Synthetic Data Generation with User Modeling

Generate multi-turn training data by simulating user behavior with LLM-driven personas.

:::{card}

**Goal**: Generate synthetic multi-turn training data using LLM-driven personas.

^^^

**In this tutorial, you will**:

1. Configure user simulation prompts
2. Generate synthetic conversations at scale
3. Process rollouts for training

:::

:::{button-ref} rlhf-reward-models
:color: secondary
:outline:
:ref-type: doc

← Previous: RLHF with Reward Models
:::

---

## What You'll Need

Install required packages:

```bash
pip install datasets openai
```

Set up API credentials (choose one):

```bash
# Option A: OpenAI API
export OPENAI_API_KEY="your-openai-key"  # pragma: allowlist secret

# Option B: NVIDIA NIMs
export NIMS_API_KEY="your-nims-key"  # pragma: allowlist secret

# Option C: Local vLLM (no key needed, but server must be running)
# vLLM serves an OpenAI-compatible API at http://localhost:8000
```

---

## What is User Modeling?

User modeling generates synthetic training data by having an LLM simulate realistic user responses. Instead of collecting human conversations, you:

1. Define **user personas** that describe realistic user characteristics
2. Create a **user simulation prompt** that instructs the LLM to act as that user
3. Build a **conversation loop** that alternates between simulated user messages and assistant responses

The `calendar/` resources server demonstrates this pattern for generating scheduling assistant training data.

:::{note}
**Key Terms**

- **Resources server**: A NeMo Gym component that provides task data, executes tool calls, and computes rewards. See {doc}`/tutorials/creating-resource-server`.
- **Rollouts**: Complete conversation trajectories generated by having the model respond to prompts. Used for training and evaluation.
- **exp_cal_state**: "Expected calendar state" — the ground truth used to verify if the model's response is correct.
:::

### Benefits

- **Scalable**: Generate thousands of conversations without human annotators
- **Diverse**: Personas create varied interaction styles and complexity levels
- **Controllable**: Adjust conversation parameters (length, complexity, topic coverage)
- **Verifiable**: Ground truth is known, enabling automatic reward computation

---

## Reference Implementation

The `calendar/` resources server demonstrates this pattern. Implementation: `resources_servers/calendar/create_synth_conversations.py`.

### Data Generation Pipeline

```text
Step 1: Generate Synthetic Conversations (create_synth_conversations.py)
    └── Uses personas from nvidia/Nemotron-Personas-USA
    └── LLM simulates user adding events and constraints

Step 2: Generate Model Rollouts (generate_rollouts.py)
    └── Model generates responses to conversations
    └── Grades responses using verification logic

Step 3: Preprocess for Training (dataset_preprocess.py)
    └── Converts to JSONL training format
    └── Splits into train/validation sets
```

---

## User Simulation Prompt

The prompt instructs the LLM to generate user messages based on persona and context. From `prompts.py`:

```python
USER_AGENT_PROMPT = """
ROLE: You are roleplaying as a user with the following persona.
PERSONA: {persona}

TASK: Generate the next user message in a conversation with a calendar assistant.

PARTIAL CONVERSATION HISTORY:
{partial_conversation_history}

YOUR GOAL FOR THIS MESSAGE:
{agenda}

REQUIREMENTS:
1. Stay in character with your persona
2. Sound natural and conversational (contractions, casual language OK)
3. Only include information relevant to the AGENDA
4. Don't mention any days of the week in your messages.
6. Respond ONLY with valid JSON in this exact format:
{{
"USER_MESSAGE": "your message here"
}}
"""
```

**Key elements**:

| Element | Purpose |
|---------|---------|
| `PERSONA` | Defines user characteristics and communication style |
| `PARTIAL CONVERSATION HISTORY` | Provides context from recent exchanges (typically last 4 messages) |
| `AGENDA` | Specifies what the user should accomplish in this turn (e.g., "add an event", "specify a constraint") |
| `REQUIREMENTS` | Constrains output format and style |

---

## Persona Sources

The calendar implementation uses [nvidia/Nemotron-Personas-USA](https://huggingface.co/datasets/nvidia/Nemotron-Personas-USA) from Hugging Face:

```python
from datasets import load_dataset

def get_personas(offset, n_samples, ds_name="nvidia/Nemotron-Personas-USA", n_traits=None):
    personas = []
    if ds_name == "nvidia/Nemotron-Personas-USA":
        ds = load_dataset(ds_name, split=f"train[{offset}:{offset + n_samples}]")
        for sample in ds:
            persona = f"PERSONA: {sample['professional_persona']}"
            personas.append(persona)
    return personas
```

Each persona provides professional context that shapes how the simulated user communicates. For example:

- A construction foreman might request events like "Daily safety briefing" and "Mentorship session with new crew member"
- A lab technician might schedule "GC-MS calibration" and "SOP development workshop"

---

## Conversation Loop

The loop alternates between LLM-generated user messages and fixed assistant responses:

```python
AGENDA_OPTIONS = {
    "add_event": 'Ask to add the following event to calendar: "{event_name}". '
                 'Mention the duration as {duration} minutes. Don\'t mention the '
                 'exact time, just the duration. Additionally, mention the event '
                 'id as {event_id} in brackets (e.g. (event id: 2))',
    "add_constraint": 'Ask for the "{event_name}" event to be scheduled with '
                      'the following constraint: {constraint}. (Note: The event '
                      'is already in the calendar. You\'re just providing a time '
                      'constraint for the event.)',
    "continue_conversation": "Continue the conversation based on your persona. "
                             "DO NOT ask any questions or try to schedule any events. "
                             "Stating things about your work/day/feelings/hopes etc "
                             "are fine.",
}
```

:::{tip}
The prompts above are simplified for readability. The actual implementation includes additional instructions (e.g., avoiding mentions of events that might trigger extra scheduling). See `create_synth_conversations.py` for the complete prompts.
:::

### Conversation Flow

1. **Add event**: User asks to schedule an event with a duration
2. **Fixed assistant response**: Confirms event was added
3. **Add constraint** (probabilistic): User specifies time constraints
4. **Continue conversation** (probabilistic): User adds persona-relevant small talk
5. Repeat until all events and constraints are specified

### Variety Parameters

Two randomized parameters control conversation variety:

```python
import random

# Probability of adding small talk after events (0.6 to 1.0)
# Higher values = more natural, longer conversations
smalltalk_factor = random.uniform(0.6, 1.0)

# Probability of adding a constraint after an event (0.6 to 1.0)
# Higher values = more complex scheduling requirements
constraint_eagerness = random.uniform(0.6, 1.0)
```

---

## Running the Pipeline

### Step 1: Generate Conversations

```bash
cd resources_servers/calendar

python create_synth_conversations.py \
    --n-samples 100 \
    --n-workers 10 \
    --n-events 5 \
    --min-time 600 \
    --max-time 960 \
    --model "openai/gpt-4o-mini" \
    --endpoint vllm \
    --ds-name "nvidia/Nemotron-Personas-USA" \
    --output ./data/synthetic_conversations.json
```

:::{note}
**Model and Endpoint Explained**

- `--model "openai/gpt-4o-mini"`: The model identifier. For vLLM, this is the model you're serving locally.
- `--endpoint vllm`: Use a local vLLM server (OpenAI-compatible API at `localhost:8000`).
- `--endpoint nims`: Use NVIDIA NIMs API (requires `NIMS_API_KEY`).

The model string format depends on your endpoint. vLLM uses the model name you started the server with.
:::

**Parameters**:

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--n-samples` | Number of conversations to generate | 2000 |
| `--n-events` | Events per conversation | 7 |
| `--min-time` | Earliest event time (minutes from midnight) | 600 (10am) |
| `--max-time` | Latest event end time | 960 (4pm) |
| `--endpoint` | `vllm` for local or `nims` for NVIDIA API | vllm |
| `--n-workers` | Parallel API calls (watch rate limits!) | 100 |

**Expected output**: A JSON file with conversation structures. For 100 samples, expect ~1-5 minutes depending on API latency.

### Step 2: Generate Model Rollouts

```bash
python generate_rollouts.py \
    --input ./data/synthetic_conversations.json \
    --output ./data/rollouts.json \
    --model "Qwen/Qwen3-8B" \
    --n-workers 50
```

**Expected output**: Rollouts with grade (0 or 1) indicating if the model correctly scheduled events.

### Step 3: Preprocess for Training

```bash
python dataset_preprocess.py \
    --input ./data/rollouts.json \
    --output_train ./data/train.jsonl \
    --output_val ./data/validation.jsonl \
    --n_val 128 \
    --exclude_success  # Only include failed rollouts for training
```

**Expected output**: Two JSONL files ready for NeMo RL training.

---

## Output Format

The pipeline produces JSONL files ready for training:

```json
{
  "responses_create_params": {
    "input": [
      {"role": "system", "content": "You are a scheduling assistant..."},
      {"role": "user", "content": "Hey, could you add a team meeting for 60 minutes? (event id: 0)"},
      {"role": "assistant", "content": "[{\"event_id\": 0, ...}]"},
      {"role": "user", "content": "Could you slot that meeting for after 2pm?"}
    ]
  },
  "exp_cal_state": {
    "0": {
      "event_id": 0,
      "duration": 60,
      "constraint": "after 2pm",
      "min_time": "10:00",
      "max_time": "16:00"
    }
  }
}
```

The `exp_cal_state` field contains the expected calendar state — ground truth for computing rewards during training.

---

## Complete Minimal Example

Copy-paste this standalone script to try user modeling:

```python
"""Minimal user modeling example - generates 5 synthetic conversations."""
import json
import random
from openai import OpenAI

# Initialize client (works with OpenAI, vLLM, or any OpenAI-compatible API)
client = OpenAI(
    api_key="your-key",  # Or use OPENAI_API_KEY env var  # pragma: allowlist secret
    base_url="https://api.openai.com/v1",  # Or http://localhost:8000/v1 for vLLM
)

# Simple personas (in production, use nvidia/Nemotron-Personas-USA)
PERSONAS = [
    "A busy marketing manager who speaks concisely",
    "A detail-oriented engineer who asks clarifying questions",
    "A friendly teacher who uses casual language",
]

USER_SIMULATION_PROMPT = """
ROLE: You are roleplaying as a user with this persona: {persona}

TASK: Generate the next user message asking a calendar assistant to schedule an event.

EVENT TO SCHEDULE: {event_name} for {duration} minutes

REQUIREMENTS:
1. Stay in character
2. Be natural and conversational
3. Respond with JSON: {{"USER_MESSAGE": "your message"}}
"""

def generate_user_message(persona: str, event_name: str, duration: int) -> str:
    """Generate a simulated user message."""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": USER_SIMULATION_PROMPT.format(
                persona=persona,
                event_name=event_name,
                duration=duration,
            )
        }],
        temperature=0.7,
    )
    content = response.choices[0].message.content
    return json.loads(content)["USER_MESSAGE"]


def generate_conversation(persona: str, events: list[dict]) -> list[dict]:
    """Generate a complete conversation for one persona."""
    conversation = [
        {"role": "system", "content": "You are a helpful calendar assistant."}
    ]
    
    for event in events:
        # Generate user message via LLM
        user_msg = generate_user_message(persona, event["name"], event["duration"])
        conversation.append({"role": "user", "content": user_msg})
        
        # Fixed assistant response (in production, use actual model response)
        conversation.append({
            "role": "assistant", 
            "content": f"Done! I've added {event['name']} to your calendar."
        })
    
    return conversation


if __name__ == "__main__":
    # Generate 5 conversations
    events = [
        {"name": "Team standup", "duration": 15},
        {"name": "Project review", "duration": 60},
    ]
    
    results = []
    for i, persona in enumerate(PERSONAS[:5]):
        print(f"Generating conversation {i+1}...")
        conv = generate_conversation(persona, events)
        results.append({"persona": persona, "conversation": conv})
    
    # Save output
    with open("synthetic_conversations.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"Generated {len(results)} conversations → synthetic_conversations.json")
```

Run with:

```bash
export OPENAI_API_KEY="your-key"  # pragma: allowlist secret
python minimal_user_modeling.py
```

---

## Adapting for Your Environment

Apply this pattern to your own resources server by customizing four components:

### 1. Define Your Persona Source

Choose or create personas relevant to your domain:

```python
from datasets import load_dataset

# Option A: Use Nemotron-Personas-USA (general professional personas)
ds = load_dataset("nvidia/Nemotron-Personas-USA", split="train[:100]")
personas = [sample["professional_persona"] for sample in ds]

# Option B: Define domain-specific personas
PERSONAS = [
    "A software engineer debugging a production issue",
    "A product manager planning a feature launch",
    "A data scientist exploring a new dataset",
]
```

### 2. Create Your User Simulation Prompt

Adapt the prompt structure for your task:

```python
USER_SIMULATION_PROMPT = """
ROLE: You are roleplaying as a user with the following persona.
PERSONA: {persona}

TASK: Generate the next user message in a conversation with [YOUR ASSISTANT TYPE].

PARTIAL CONVERSATION HISTORY:
{partial_conversation_history}

YOUR GOAL FOR THIS MESSAGE:
{agenda}

REQUIREMENTS:
1. Stay in character with your persona
2. Sound natural and conversational
3. [YOUR DOMAIN-SPECIFIC REQUIREMENTS]

Respond ONLY with valid JSON:
{{"USER_MESSAGE": "your message here"}}
"""
```

### 3. Define Agenda Options

Specify the types of actions users can take:

```python
AGENDA_OPTIONS = {
    "primary_action": "Description of main user task...",
    "add_detail": "Add additional information to previous request...",
    "clarify": "Ask for clarification about the assistant's response...",
    "change_request": "Request a modification to previous output...",
}
```

### 4. Build Your Conversation Loop

```python
import random
from typing import Any

def generate_user_message(
    persona: str, 
    conversation: list[dict], 
    agenda: str, 
    model: Any
) -> str:
    """Generate a user message using the LLM."""
    # Get last 4 messages for context
    history = conversation[-4:] if len(conversation) >= 4 else conversation
    history_str = "\n".join(f"{m['role']}: {m['content']}" for m in history)
    
    prompt = USER_SIMULATION_PROMPT.format(
        persona=persona,
        partial_conversation_history=history_str,
        agenda=agenda,
    )
    response = model.generate(prompt)
    return response["USER_MESSAGE"]


def generate_conversation(
    persona: str, 
    task_spec: dict, 
    model: Any
) -> list[dict]:
    """Generate a complete conversation."""
    conversation = []
    
    for step in task_spec["steps"]:
        # Generate user message
        agenda = AGENDA_OPTIONS[step["type"]].format(**step["params"])
        user_msg = generate_user_message(persona, conversation, agenda, model)
        conversation.append({"role": "user", "content": user_msg})
        
        # Generate or use fixed assistant response
        if step.get("fixed_response"):
            conversation.append({"role": "assistant", "content": step["fixed_response"]})
    
    return conversation
```

---

## Quality Control

Validate generated data to ensure training quality.

### Conversation Validation

```python
def _validate_events(events: list[dict], min_time: int, max_time: int) -> bool:
    """Validate events have no overlaps and are within time window."""
    for i in range(len(events)):
        start_i, end_i = parse_event_times(events[i])
        
        # Check time window
        if start_i < min_time or end_i > max_time:
            return False
        
        # Check for overlaps
        for j in range(i + 1, len(events)):
            start_j, end_j = parse_event_times(events[j])
            if not (end_i <= start_j or start_i >= end_j):
                return False
    
    return True
```

### Constraint Satisfaction

Each generated event must satisfy its constraint:

```python
def _check_constraint_satisfied(event: dict) -> bool:
    """Check if an event satisfies its constraint."""
    start_time, end_time = parse_event_times(event)
    constraint = event["constraint"]
    
    if constraint.startswith("before "):
        return end_time <= parse_time(constraint.replace("before ", ""))
    elif constraint.startswith("after "):
        return start_time >= parse_time(constraint.replace("after ", ""))
    # ... handle other constraint types
```

---

## Production Considerations

### Cost Estimation

| Scale | Estimated Calls | Cost (GPT-4o-mini) |
|-------|-----------------|-------------------|
| 100 samples, 5 events | ~1,500 calls | ~$0.30 |
| 1,000 samples, 7 events | ~21,000 calls | ~$4.20 |
| 10,000 samples, 7 events | ~210,000 calls | ~$42 |

*Estimates based on GPT-4o-mini pricing (~$0.15/1M input tokens, ~300 tokens/call). Actual costs vary by model, provider, and conversation complexity. Local vLLM deployments have no API costs but require GPU resources.*

### Rate Limiting

The `--n-workers` parameter controls parallelism. High values may trigger rate limits:

```bash
# Conservative: Won't hit rate limits
python create_synth_conversations.py --n-workers 10

# Aggressive: May need retry logic
python create_synth_conversations.py --n-workers 100
```

### Long-Running Jobs

For large generation runs (>1000 samples):

1. **Save intermediate results**: The scripts save on completion. For crash recovery, run in smaller batches.
2. **Monitor progress**: Output shows a progress bar with completion estimates.
3. **Disk space**: Each conversation is ~2-5KB. 10,000 samples ≈ 50MB.

### API Key Security

Never commit API keys to version control:

```bash
# Good: Use environment variables
export OPENAI_API_KEY="sk-..."  # pragma: allowlist secret

# Bad: Don't put keys in scripts
api_key = "sk-..."  # NEVER DO THIS  # pragma: allowlist secret
```

---

## Next Steps

::::{grid} 1 2 2 2
:gutter: 3

:::{grid-item-card} {octicon}`iterations;1.5em;sd-mr-1` Multi-Turn Environments
:link: multi-turn
:link-type: doc
Learn the multi-turn patterns that user modeling generates data for.
:::

:::{grid-item-card} {octicon}`rocket;1.5em;sd-mr-1` Train with NeMo RL
:link: training-nemo-rl-grpo-index
:link-type: ref
Train models on your synthetic data.
:::

:::{grid-item-card} {octicon}`database;1.5em;sd-mr-1` Prepare Data
:link: /data/prepare-validate
:link-type: doc
Validate your generated datasets.
:::

:::{grid-item-card} {octicon}`code;1.5em;sd-mr-1` Calendar Implementation
:link: https://github.com/NVIDIA/NeMo-Gym/tree/main/resources_servers/calendar
Reference implementation for user modeling.
:::

::::
